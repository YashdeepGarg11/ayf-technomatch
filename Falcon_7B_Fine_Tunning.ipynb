{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0zRdnBOgwGfo"
      },
      "outputs": [],
      "source": [
        "# Install necessary packages\n",
        "!pip install -q -U torch==2.0.1 bitsandbytes==0.40.2\n",
        "!pip install -q -U transformers==4.31.0 peft==0.4.0 accelerate==0.21.0\n",
        "!pip install -q -U datasets py7zr einops transformers\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GiBvvZt7v7nZ"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "from transformers import AdamW, get_linear_schedule_with_warmup\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
        "from peft import prepare_model_for_kbit_training, LoraConfig\n",
        "from datasets import load_dataset\n",
        "import transformers\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YK9hM-tCwJN2"
      },
      "outputs": [],
      "source": [
        "cuda_install_dir = '/'.join(nvidia.__file__.split('/')[:-1]) + '/cuda_runtime/lib/'\n",
        "os.environ['LD_LIBRARY_PATH'] = cuda_install_dir\n",
        "\n",
        "model_id = \"tiiuae/falcon-7b\"\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16\n",
        ")\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BU5TRUXGwRBW"
      },
      "outputs": [],
      "source": [
        "model = AutoModelForCausalLM.from_pretrained(model_id, trust_remote_code=True, quantization_config=bnb_config, device_map=\"auto\")\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "model.gradient_checkpointing_enable()\n",
        "model = prepare_model_for_kbit_training(model)\n",
        "optimizer = AdamW(model.parameters(), lr=2e-4)\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=len(lm_train_dataset))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aIA3UuktwYr3"
      },
      "outputs": [],
      "source": [
        "def print_trainable_parameters(model):\n",
        "    trainable_params = 0\n",
        "    all_param = 0\n",
        "    for _, param in model.named_parameters():\n",
        "        all_param += param.numel()\n",
        "        if param.requires_grad:\n",
        "            trainable_params += param.numel()\n",
        "    print(\n",
        "        f\"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param}\"\n",
        "    )\n",
        "\n",
        "config = LoraConfig(\n",
        "    r=8,\n",
        "    lora_alpha=32,\n",
        "    target_modules=[\n",
        "        \"query_key_value\",\n",
        "        \"dense\",\n",
        "        \"dense_h_to_4h\",\n",
        "        \"dense_4h_to_h\",\n",
        "    ],\n",
        "    lora_dropout=0.05,\n",
        "    task_type=\"CAUSAL_LM\"\n",
        ")\n",
        "model = get_peft_model(model, config)\n",
        "print_trainable_parameters(model)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ilMh5xtewb4F"
      },
      "outputs": [],
      "source": [
        "dataset = load_dataset(\"os/data.json\")\n",
        "\n",
        "print(f\"Train dataset size: {len(dataset['train'])}\")\n",
        "print(f\"Test dataset size: {len(dataset['test'])}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pX4MUsX5whqG"
      },
      "outputs": [],
      "source": [
        "from random import randint\n",
        "\n",
        "prompt_template = f\"Perform Writing Style Evaluation On: {{dialogue}} {{summary}}\"\n",
        "\n",
        "def template_dataset(sample):\n",
        "    sample[\"text\"] = prompt_template.format(dialogue=sample[\"dialogue\"],\n",
        "                                            summary=sample[\"summary\"],\n",
        "                                            eos_token=tokenizer.eos_token)\n",
        "    return sample\n",
        "\n",
        "train_dataset = dataset[\"train\"].map(template_dataset, remove_columns=list(dataset[\"train\"].features))\n",
        "\n",
        "print(train_dataset[randint(0, len(dataset))][\"text\"])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pwAa6NTAwuoN"
      },
      "outputs": [],
      "source": [
        "test_dataset = dataset[\"test\"].map(template_dataset, remove_columns=list(dataset[\"test\"].features))\n",
        "\n",
        "lm_train_dataset = train_dataset.map(\n",
        "    lambda sample: tokenizer(sample[\"text\"]), batched=True, batch_size=24, remove_columns=list(train_dataset.features)\n",
        ")\n",
        "\n",
        "lm_test_dataset = test_dataset.map(\n",
        "    lambda sample: tokenizer(sample[\"text\"]), batched=True, remove_columns=list(test_dataset.features)\n",
        ")\n",
        "\n",
        "print(f\"Total number of train samples: {len(lm_train_dataset)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BmQ74cp5w65T"
      },
      "outputs": [],
      "source": [
        "bucket = \"localhost:127.0.0.1\"\n",
        "log_bucket = f\"{bucket}/falcon-7b-qlora-finetune\"\n",
        "\n",
        "num_epochs = 120 # tarin steps\n",
        "batch_size = 15 \n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    dataloader = DataLoader(lm_train_dataset, batch_size=batch_size, shuffle=True)\n",
        "    for batch in dataloader:\n",
        "        inputs = {k: v.to(model.device) for k, v in batch.items()}\n",
        "        outputs = model(**inputs)\n",
        "        loss = outputs.loss\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        # Backpropagation\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        scheduler.step()\n",
        "\n",
        "    # Print average loss for each epoch\n",
        "    avg_loss = total_loss / len(dataloader)\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs}, Average Loss: {avg_loss}\")\n",
        "\n",
        "\n",
        "trainer = transformers.Trainer(\n",
        "    model=model,\n",
        "    train_dataset=lm_train_dataset,\n",
        "    eval_dataset=lm_test_dataset,\n",
        "    args=transformers.TrainingArguments(\n",
        "        per_device_train_batch_size=8,\n",
        "        per_device_eval_batch_size=8,\n",
        "        logging_dir=log_bucket,\n",
        "        logging_steps=2,\n",
        "        num_train_epochs=1,\n",
        "        learning_rate=2e-4,\n",
        "        bf16=True,\n",
        "        save_strategy=\"no\",\n",
        "        output_dir=\"outputs\",\n",
        "        report_to=\"tensorboard\",\n",
        "    ),\n",
        "    data_collator=transformers.DataCollatorForLanguageModeling(tokenizer, mlm=False),\n",
        ")\n",
        "\n",
        "model.config.use_cache = False\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q8xHYgtvxBwf"
      },
      "outputs": [],
      "source": [
        "trainer.train()\n",
        "trainer.evaluate()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "plosRMpgxDKv"
      },
      "outputs": [],
      "source": [
        "test_dataset = load_dataset()\n",
        "\n",
        "sample = test_dataset[randint(0, len(test_dataset))]\n",
        "\n",
        "prompt_template = f\"Perform Sentimental Analysis On: {{dialogue}}\"\n",
        "\n",
        "test_sample = prompt_template.format(dialogue=sample[\"dialogue\"])\n",
        "\n",
        "print(test_sample)\n",
        "\n",
        "input_ids = tokenizer(test_sample, return_tensors=\"pt\").input_ids\n",
        "tokens_for_summary = 1024\n",
        "output_tokens = input_ids.shape[1] + tokens_for_summary\n",
        "\n",
        "outputs = model.generate(inputs=input_ids, do_sample=True, max_length=output_tokens)\n",
        "gen_text = tokenizer.batch_decode(outputs)[0]\n",
        "print(gen_text)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
